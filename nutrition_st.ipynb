{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sreejavepa/Nutrition-LLM/blob/main/nutrition_st.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gw0Lcw2nN3b9",
        "outputId": "7ec729f7-f930-4540-bfcb-999524c92d59"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: pyngrok in /usr/local/lib/python3.11/dist-packages (7.3.0)\n",
            "Requirement already satisfied: PyYAML>=5.1 in /usr/local/lib/python3.11/dist-packages (from pyngrok) (6.0.2)\n",
            "Requirement already satisfied: streamlit in /usr/local/lib/python3.11/dist-packages (1.48.0)\n",
            "Requirement already satisfied: altair!=5.4.0,!=5.4.1,<6,>=4.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (5.5.0)\n",
            "Requirement already satisfied: blinker<2,>=1.5.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (1.9.0)\n",
            "Requirement already satisfied: cachetools<7,>=4.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (5.5.2)\n",
            "Requirement already satisfied: click<9,>=7.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (8.2.1)\n",
            "Requirement already satisfied: numpy<3,>=1.23 in /usr/local/lib/python3.11/dist-packages (from streamlit) (2.0.2)\n",
            "Requirement already satisfied: packaging<26,>=20 in /usr/local/lib/python3.11/dist-packages (from streamlit) (25.0)\n",
            "Requirement already satisfied: pandas<3,>=1.4.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (2.2.2)\n",
            "Requirement already satisfied: pillow<12,>=7.1.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (11.3.0)\n",
            "Requirement already satisfied: protobuf<7,>=3.20 in /usr/local/lib/python3.11/dist-packages (from streamlit) (5.29.5)\n",
            "Requirement already satisfied: pyarrow>=7.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (18.1.0)\n",
            "Requirement already satisfied: requests<3,>=2.27 in /usr/local/lib/python3.11/dist-packages (from streamlit) (2.32.3)\n",
            "Requirement already satisfied: tenacity<10,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (8.5.0)\n",
            "Requirement already satisfied: toml<2,>=0.10.1 in /usr/local/lib/python3.11/dist-packages (from streamlit) (0.10.2)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.4.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (4.14.1)\n",
            "Requirement already satisfied: watchdog<7,>=2.1.5 in /usr/local/lib/python3.11/dist-packages (from streamlit) (6.0.0)\n",
            "Requirement already satisfied: gitpython!=3.1.19,<4,>=3.0.7 in /usr/local/lib/python3.11/dist-packages (from streamlit) (3.1.45)\n",
            "Requirement already satisfied: pydeck<1,>=0.8.0b4 in /usr/local/lib/python3.11/dist-packages (from streamlit) (0.9.1)\n",
            "Requirement already satisfied: tornado!=6.5.0,<7,>=6.0.3 in /usr/local/lib/python3.11/dist-packages (from streamlit) (6.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from altair!=5.4.0,!=5.4.1,<6,>=4.0->streamlit) (3.1.6)\n",
            "Requirement already satisfied: jsonschema>=3.0 in /usr/local/lib/python3.11/dist-packages (from altair!=5.4.0,!=5.4.1,<6,>=4.0->streamlit) (4.25.0)\n",
            "Requirement already satisfied: narwhals>=1.14.2 in /usr/local/lib/python3.11/dist-packages (from altair!=5.4.0,!=5.4.1,<6,>=4.0->streamlit) (2.0.1)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.11/dist-packages (from gitpython!=3.1.19,<4,>=3.0.7->streamlit) (4.0.12)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas<3,>=1.4.0->streamlit) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas<3,>=1.4.0->streamlit) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas<3,>=1.4.0->streamlit) (2025.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.27->streamlit) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.27->streamlit) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.27->streamlit) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.27->streamlit) (2025.8.3)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.11/dist-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.19,<4,>=3.0.7->streamlit) (5.0.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->altair!=5.4.0,!=5.4.1,<6,>=4.0->streamlit) (3.0.2)\n",
            "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=3.0->altair!=5.4.0,!=5.4.1,<6,>=4.0->streamlit) (25.3.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=3.0->altair!=5.4.0,!=5.4.1,<6,>=4.0->streamlit) (2025.4.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=3.0->altair!=5.4.0,!=5.4.1,<6,>=4.0->streamlit) (0.36.2)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=3.0->altair!=5.4.0,!=5.4.1,<6,>=4.0->streamlit) (0.26.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas<3,>=1.4.0->streamlit) (1.17.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install -q rapidfuzz faiss-cpu streamlit\n",
        "!pip install pyngrok\n",
        "!pip install --upgrade streamlit\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XQt7w6W7wxhm",
        "outputId": "ff8259d6-869c-45bc-bc60-75b987006eeb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Authtoken saved to configuration file: /root/.config/ngrok/ngrok.yml\n"
          ]
        }
      ],
      "source": [
        "!ngrok authtoken YOUR_AUTH_TOKEN\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Kp-RmELPuUve"
      },
      "outputs": [],
      "source": [
        "from pyngrok import ngrok\n",
        "ngrok.kill()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cyl9qkZqMipM",
        "outputId": "205875b8-eebe-48d4-975d-54194715ceab"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Overwriting nutrition_app.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile nutrition_app.py\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import warnings\n",
        "import ast\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "import streamlit as st\n",
        "\n",
        "from sentence_transformers import SentenceTransformer, util\n",
        "import faiss\n",
        "from rapidfuzz import process, fuzz\n",
        "\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "import torch\n",
        "from huggingface_hub import login\n",
        "\n",
        "# load data\n",
        "\n",
        "@st.cache_data\n",
        "def load_data():\n",
        "  usda = pd.read_csv('USDA.csv')\n",
        "  usda['clean_description'] = usda['Description'].str.lower().str.strip()\n",
        "\n",
        "  intake = pd.read_csv('intake.csv', header=[0,1])\n",
        "\n",
        "  # remove commas in intake numeric columns\n",
        "  def remove_commas(x):\n",
        "      if isinstance(x, str):\n",
        "          return x.replace(',', '')\n",
        "      else:\n",
        "          return x\n",
        "  intake.iloc[:, 2:] = intake.iloc[:, 2:].applymap(remove_commas)\n",
        "\n",
        "  recipes = pd.read_csv('world_recipes.csv')\n",
        "\n",
        "  # rename columns for consistency\n",
        "  match_dict = {\n",
        "      \"TotalFat\": \"Fat\",\n",
        "      \"VitaminC\": \"Vitamin C\",\n",
        "      \"VitaminE\": \"Vitamin E\",\n",
        "      \"VitaminD\": \"Vitamin D\"\n",
        "  }\n",
        "  usda.rename(columns=match_dict, inplace=True)\n",
        "\n",
        "  # clean 'Life Stage' column\n",
        "  intake[('Life Stage', 'units')] = intake[('Life Stage', 'units')].apply(\n",
        "      lambda s: s.replace('\\u2003', ' ').replace('\\xa0', ' ').strip()\n",
        "  )\n",
        "  return usda, intake, recipes\n",
        "\n",
        "usda, intake, recipes = load_data()\n",
        "\n",
        "\n",
        "@st.cache_resource\n",
        "def build_faiss_index(recipes):\n",
        "  embedder = SentenceTransformer('paraphrase-MiniLM-L3-v2')\n",
        "  recipe_names = recipes['recipe'].astype(str).tolist()\n",
        "  embeddings = embedder.encode(recipe_names, convert_to_tensor=True)\n",
        "  embeddings = embeddings / np.linalg.norm(embeddings, axis=1, keepdims=True)\n",
        "  dim = embeddings.shape[1]\n",
        "  index = faiss.IndexFlatL2(dim)\n",
        "  index.add(embeddings)\n",
        "  return embedder, index, embeddings\n",
        "\n",
        "embedder, index, embeddings = build_faiss_index(recipes)\n",
        "\n",
        "\n",
        "def get_recipe(query, top_k=2):\n",
        "    query_embedding = embedder.encode([query], convert_to_numpy=True)\n",
        "    query_embedding = query_embedding / np.linalg.norm(query_embedding, axis=1, keepdims=True)\n",
        "    dist, ind = index.search(query_embedding, top_k)\n",
        "    results = []\n",
        "    for idx, score in zip(ind[0], dist[0]):\n",
        "        row = recipes.iloc[idx]\n",
        "        results.append({'item': row['recipe'], 'ingredients': row['ingredients'], 'score': score})\n",
        "    return results\n",
        "\n",
        "def combine_ingredients(results):\n",
        "    comb_ingredients = set()\n",
        "    for r in results:\n",
        "        ingredients = ast.literal_eval(r['ingredients'].lower())\n",
        "        comb_ingredients.update([i.strip() for i in ingredients])\n",
        "    return list(comb_ingredients)\n",
        "\n",
        "def get_top_candidate(ingredient, top_k=5, threshold=70):\n",
        "    usda_list = usda['clean_description'].tolist()\n",
        "    candidates = process.extract(ingredient, usda_list, scorer=fuzz.partial_ratio, limit=top_k)\n",
        "    best_cand = max(candidates, key=lambda x: x[1])\n",
        "    if best_cand[1] >= threshold:\n",
        "        return best_cand[0]\n",
        "    else:\n",
        "        return None\n",
        "\n",
        "def calculate_nutrition(matched_df, intake, group, life_stage, nutrient_cols):\n",
        "    # Get relevant intake row\n",
        "    row = intake[(intake[('Group', 'Unnamed: 0_level_1')] == group) & (intake[('Life Stage', 'units')] == life_stage)]\n",
        "    if row.empty:\n",
        "        raise ValueError(\"No matching intake data for selected group and life stage\")\n",
        "    row = row.iloc[0]\n",
        "\n",
        "    full_nutrient_cols = [idx for idx in row.index if idx[0] in nutrient_cols]\n",
        "\n",
        "    # Calculate total nutrients from matched_df\n",
        "    total_nutrients = matched_df[nutrient_cols].sum()\n",
        "    req_values = row[full_nutrient_cols].replace(['ND', 'NDc', 'NaN', None], 0).astype(float)\n",
        "\n",
        "    total_nutrients.index = pd.MultiIndex.from_tuples([(nutrient, '') for nutrient in total_nutrients.index])\n",
        "    totals = pd.Series({idx: total_nutrients.get((idx[0], ''), 0) for idx in full_nutrient_cols})\n",
        "\n",
        "    coverage = (totals / req_values) * 100\n",
        "    return totals, coverage\n",
        "\n",
        "def build_coverage_df(coverage):\n",
        "    coverage_df = pd.DataFrame(coverage, columns=['Percent(%) Daily'])\n",
        "    coverage_df = coverage_df.reset_index()\n",
        "    coverage_df = coverage_df.rename(columns={'level_0': 'Nutrient', 'level_1': 'Unit'})\n",
        "    return coverage_df\n",
        "\n",
        "def retrieve(query, docs):\n",
        "    query = query.lower()\n",
        "    keywords = query.split()\n",
        "    results = [doc for doc in docs if any(kw in doc.lower() for kw in keywords)]\n",
        "    return results if results else [\"No relevant information found.\"]\n",
        "\n",
        "# LLaMa MODEL SETUP\n",
        "\n",
        "@st.cache_resource\n",
        "def load_llama_model():\n",
        "    login(token=\"YOUR_AUTH_TOKEN")\n",
        "    model_id = \"meta-llama/Llama-3.1-8B-Instruct\"\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
        "    model = AutoModelForCausalLM.from_pretrained(\n",
        "        model_id,\n",
        "        torch_dtype=torch.float16,\n",
        "        device_map=\"auto\"\n",
        "    )\n",
        "    return tokenizer, model\n",
        "\n",
        "tokenizer, model = load_llama_model()\n",
        "\n",
        "def generate_response(user_input, docs):\n",
        "    ret_info = retrieve(user_input, docs)\n",
        "    context = \"\\n\".join(ret_info)\n",
        "    prompt = f\"Given the {context}, briefly answer the question: \\n{user_input}.\"\n",
        "\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
        "    outputs = model.generate(\n",
        "        **inputs,\n",
        "        max_new_tokens=150,\n",
        "        do_sample=True,\n",
        "        temperature=0.7,\n",
        "        top_p=0.9,\n",
        "        pad_token_id=tokenizer.eos_token_id\n",
        "    )\n",
        "    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "\n",
        "    if response.startswith(prompt):\n",
        "      answer = response[len(prompt):].strip()\n",
        "    else:\n",
        "      answer = full_response.strip()\n",
        "\n",
        "    return answer\n",
        "\n",
        "# STREAMLIT UI\n",
        "\n",
        "st.title(\"Nutrition Explorer\")\n",
        "\n",
        "user_input = st.text_input(\"What food would you like to look at today?\").strip().lower()\n",
        "\n",
        "if user_input:\n",
        "    with st.spinner('Finding recipes...'):\n",
        "        recipes_found = get_recipe(user_input)\n",
        "    st.subheader(\"Top Recipes Found\")\n",
        "    for i, r in enumerate(recipes_found, 1):\n",
        "        st.markdown(f\"**{i}. {r['item']}** (Score: {r['score']:.4f})\")\n",
        "\n",
        "    all_ingredients = combine_ingredients(recipes_found)\n",
        "\n",
        "    st.subheader(\"Ingredients (edit below)\")\n",
        "    ingredients_text = \"\\n\".join(all_ingredients)\n",
        "    edited_text = st.text_area(\"Edit ingredients (one per line):\", ingredients_text, height=200)\n",
        "    all_ingredients = [line.strip().lower() for line in edited_text.split(\"\\n\") if line.strip()]\n",
        "\n",
        "    # Match ingredients to USDA\n",
        "    matched_rows = []\n",
        "    unmatched = []\n",
        "    for ingredient in all_ingredients:\n",
        "        best_match = get_top_candidate(ingredient)\n",
        "        if best_match:\n",
        "            matched_rows.append(usda[usda['clean_description'] == best_match])\n",
        "        else:\n",
        "            unmatched.append(ingredient)\n",
        "\n",
        "    if unmatched:\n",
        "        st.warning(f\"No USDA match for: {', '.join(unmatched)}\")\n",
        "\n",
        "    if matched_rows:\n",
        "        matched_df = pd.concat(matched_rows, ignore_index=True)\n",
        "\n",
        "        matched_cols = [\"Protein\", \"Fat\", \"Carbohydrate\", \"Sodium\", \"Calcium\", \"Iron\", \"Potassium\", \"Vitamin C\", \"Vitamin E\", \"Vitamin D\"]\n",
        "\n",
        "        group_options = intake[('Group', 'Unnamed: 0_level_1')].unique()\n",
        "        group = st.selectbox(\"Which group are you part of?\", options=group_options)\n",
        "\n",
        "        life_stage_options = intake.loc[intake[('Group', 'Unnamed: 0_level_1')] == group, ('Life Stage', 'units')].unique()\n",
        "        cleaned_life_stage = [s.replace('\\u2003', ' ').replace('\\xa0', ' ').strip() for s in life_stage_options]\n",
        "        life_stage = st.selectbox(\"Which stage of life are you part of?\", options=cleaned_life_stage)\n",
        "\n",
        "        calculate = st.button(\"Calculate Nutrition\")\n",
        "\n",
        "        # Chatbot question input\n",
        "        if 'question' not in st.session_state:\n",
        "            st.session_state['question'] = ''\n",
        "        st.session_state['question'] = st.text_input(\"Ask a nutrition question based on this data:\", st.session_state['question'])\n",
        "        question = st.session_state['question']\n",
        "\n",
        "        if calculate:\n",
        "            try:\n",
        "              totals, coverage = calculate_nutrition(matched_df, intake, group, life_stage, matched_cols)\n",
        "              coverage_df = build_coverage_df(coverage)\n",
        "              st.subheader(\"Percentage of Daily Nutrient Requirements\")\n",
        "              st.dataframe(coverage_df.style.format({\"Percent(%) Daily\": \"{:.1f}%\"}))\n",
        "              st.subheader(\"Total Amounts in Food\")\n",
        "              st.dataframe(totals)\n",
        "\n",
        "              docs = []\n",
        "              for _, row in coverage_df.iterrows():\n",
        "                unit = row['Unit'] if pd.notna(row['Unit']) else ''\n",
        "                text = f\"{row['Nutrient']} {row['Unit']}: {row['Percent(%) Daily']:.1f}% of daily needs.\"\n",
        "                docs.append(text)\n",
        "              st.session_state['docs'] = docs\n",
        "\n",
        "            except Exception as e:\n",
        "              st.error(f\"Error calculating nutrition value: {e}\")\n",
        "\n",
        "        if question and question.strip():\n",
        "            if 'docs' in st.session_state:\n",
        "              with st.spinner('Generating response...'):\n",
        "                response = generate_response(question, st.session_state['docs'])\n",
        "              st.write(response)\n",
        "            else:\n",
        "              st.info(\"Click calculate Nutrition' first to generate data.\")\n",
        "\n",
        "else:\n",
        "    st.info(\"Please enter a food to get started.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "XG1HG17lzMSy",
        "outputId": "d17fb526-2e9b-44ea-a390-8700d2865e20"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "NgrokTunnel: \"https://285c3f37f592.ngrok-free.app\" -> \"http://localhost:8501\"\n",
            "\n",
            "Collecting usage statistics. To deactivate, set browser.gatherUsageStats to false.\n",
            "\u001b[0m\n",
            "\u001b[0m\n",
            "\u001b[34m\u001b[1m  You can now view your Streamlit app in your browser.\u001b[0m\n",
            "\u001b[0m\n",
            "\u001b[34m  Local URL: \u001b[0m\u001b[1mhttp://localhost:8501\u001b[0m\n",
            "\u001b[34m  Network URL: \u001b[0m\u001b[1mhttp://172.28.0.12:8501\u001b[0m\n",
            "\u001b[34m  External URL: \u001b[0m\u001b[1mhttp://34.34.88.2:8501\u001b[0m\n",
            "\u001b[0m\n",
            "2025-08-13 07:37:19.505467: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1755070639.523270   39884 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1755070639.528355   39884 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "W0000 00:00:1755070639.542141   39884 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1755070639.542188   39884 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1755070639.542191   39884 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1755070639.542193   39884 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "2025-08-13 07:37:19.546776: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "Loading checkpoint shards: 100% 4/4 [00:27<00:00,  6.98s/it]\n"
          ]
        }
      ],
      "source": [
        "from pyngrok import ngrok\n",
        "public_url = ngrok.connect(8501)\n",
        "print(public_url)\n",
        "\n",
        "!streamlit run nutrition_app.py\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "machine_shape": "hm",
      "provenance": [],
      "authorship_tag": "ABX9TyNcniP1jmEfFCJTVtyXoeDS",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
